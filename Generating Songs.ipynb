{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "\n",
    "This notebook showcases how to generate outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from collections import Counter\n",
    "from keras.models import load_model\n",
    "from gensim.models import KeyedVectors\n",
    "from textatistic import Textatistic\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "data loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "print(\"Loading data\")\n",
    "data = pd.read_csv(\"./LocalData/ProcessedSongData.csv\")\n",
    "print(\"data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for preparing user input.\n",
    "# Needs to undergo the same clean-up, and tokenization.\n",
    "def basic_cleaning(sentence):\n",
    "    s = sentence.lower()\n",
    "    s = s.replace(\"\\n\", \" \\n \")\n",
    "    return s\n",
    "\n",
    "def tokenize(s):\n",
    "    s_list = [w for w in s.split(' ') if w.strip() != '' or w == '\\r\\n']\n",
    "    for i, w in enumerate(s_list):\n",
    "        if w == '\\r\\n':\n",
    "            s_list[i] = '\\\\r\\\\n'\n",
    "    return s_list\n",
    "\n",
    "# Let us clean the token list with this new information.\n",
    "# The below removes anything except whitespace and alphanumeric characters.\n",
    "def remove_punctuation(s):\n",
    "    return re.sub('[^\\w\\s]', ' ', s)\n",
    "\n",
    "# There are, to my awareness, no words with consecutive \n",
    "# three same letters in english.\n",
    "def remove_extra_letters(s):\n",
    "    return re.sub(r\"(.)\\1{2,}\", r\"\\1\"*2, s)\n",
    "\n",
    "def clean(sentence):\n",
    "    s = basic_cleaning(sentence)\n",
    "    s = remove_punctuation(s)\n",
    "    s = remove_extra_letters(s)\n",
    "    return tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['t_corrected'] = data['corrected'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocab.\n",
      "Number of words total:  15749211\n",
      "Unique words:  61999\n"
     ]
    }
   ],
   "source": [
    "# Prep vocab\n",
    "print(\"Creating vocab.\")\n",
    "text_values = data.t_corrected.values\n",
    "vocab = Counter()\n",
    "\n",
    "text_in_words = []\n",
    "for song in text_values:\n",
    "    vocab.update(song)\n",
    "    text_in_words.extend(song)\n",
    "\n",
    "print(\"Number of words total: \", len(text_in_words))\n",
    "print(\"Unique words: \", len(vocab))\n",
    "\n",
    "vocab_keys = sorted(list(vocab.keys()))\n",
    "\n",
    "clean_songs = text_values\n",
    "\n",
    "word_indices = dict((c, i) for i, c in enumerate(vocab_keys))\n",
    "indices_word = dict((i, c) for i, c in enumerate(vocab_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\heier\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\heier\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\heier\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "MODEL_FILE = './LocalData/Final_20190530170302Type_256_256.h5'\n",
    "SEQUENCE_LEN= 10\n",
    "test_model = load_model(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Keyed Vectors.\n"
     ]
    }
   ],
   "source": [
    "# Load Keyed Vectors\n",
    "print(\"Loading Keyed Vectors.\")\n",
    "EMBEDDING_SIZE = 100\n",
    "wv = KeyedVectors.load(\"./LocalData/song_word_vec.kv\")\n",
    "\n",
    "wv['\\\\r\\\\n'] = wv['\\r\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore this for now.\n",
    "def generate_nn_data(sentence_list, next_word_list):\n",
    "    x = np.zeros((len(sentence_list), SEQUENCE_LEN, EMBEDDING_SIZE), dtype=np.float32)\n",
    "    y = np.zeros(len(next_word_list), dtype=np.int32)\n",
    "    # Go through each sentence fragment\n",
    "    for i, s in enumerate(sentence_list):\n",
    "        # For each word in the sentence fragment, get the vector\n",
    "        for t, w in enumerate(s):\n",
    "            # If word not recognized, leave blank.\n",
    "            if w in wv:\n",
    "                x[i, t, :] = wv[w]\n",
    "            else:\n",
    "                print(\"Word unrecognized: \", w)\n",
    "                \n",
    "        # Set the appropriate y-value.\n",
    "        y[i] = word_indices[next_word_list[i]]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction method\n",
    "# Functions from keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "# Give user sentence\n",
    "def predict_word(model, seed, word_num, diversity):\n",
    "    # clean up the sentence.\n",
    "    if type(seed) == str:\n",
    "        s = clean(seed)[-word_num:]\n",
    "    else:\n",
    "        s = seed\n",
    "    # Give user warning if some word is not recognized.\n",
    "    for i, w in enumerate(s):\n",
    "        if w not in wv:\n",
    "            print(\"WARNING: The word \", w, \" is not in this vocabulary.\")\n",
    "    \n",
    "    x_pred = np.zeros((1, SEQUENCE_LEN, EMBEDDING_SIZE), dtype=np.float32)\n",
    "    for t, w in enumerate(s):\n",
    "        x_pred[0, t, :] = wv[w]    \n",
    "    \n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = sample(preds, diversity)\n",
    "    next_word = indices_word[next_index]\n",
    "\n",
    "    return next_word\n",
    "\n",
    "def write_song(model, seed, song_len, word_num, diversity):\n",
    "    s = clean(seed)[-word_num:]\n",
    "    song = []\n",
    "    song.extend(s)\n",
    "    for i in range(song_len):\n",
    "        pred = predict_word(model, s, word_num, diversity)\n",
    "        song.append(pred)\n",
    "        s = s[1:]\n",
    "        s.append(pred)\n",
    "    \n",
    "    return \" \".join(song)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'end of passion play crumbling away i m your source like t \\\\r\\\\n but i ll be to you here \\\\r\\\\n i could be going \\\\r\\\\n \\\\r\\\\n you you think i re a \\\\r\\\\n all we want to be to this he s m ll up much in chorus heart \\\\r\\\\n \\\\r\\\\n the whole and \\\\r\\\\n and i need to give it the home \\\\r\\\\n of all the go \\\\r\\\\n \\\\r\\\\n yeah of a in the \\\\r\\\\n but \\\\r\\\\n i believe to i ll be how i think \\\\r\\\\n \\\\r\\\\n \\\\r\\\\n i could come \\\\r\\\\n i re your way \\\\r\\\\n come \\\\r\\\\n i m gone oh and my \\\\r\\\\n \\\\r\\\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's run some examples!\n",
    "write_song(test_model, \"End of passion play, crumbling away I'm your source\", 100, SEQUENCE_LEN, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = re.compile('(\\s){2,}')\n",
    "\n",
    "def process_output(song):\n",
    "    song = song.replace('\\\\r\\\\n', '.\\n')\n",
    "    song = pat.sub('\\n', song)\n",
    "    if song[-1] != '.':\n",
    "        song += '.'\n",
    "    return song\n",
    "\n",
    "# Scoring method!\n",
    "def score(lyrics):\n",
    "    s = Textatistic(lyrics)\n",
    "    return s.flesch_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\heier\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a b c d there s a monkey in a .\n",
      "what will am .\n",
      ".\n",
      "was for t live get ve he .\n",
      "i ain t .\n",
      "you better tell it of my things .\n",
      "and a thing so .\n",
      "and oo .\n",
      "but it and ooh baby if .\n",
      "baby but good it baby is in soon .\n",
      "it s .\n",
      ".\n",
      "they never you were out .\n",
      "don t .\n",
      "all you ll i need .\n",
      "baby i can t know you m all me in tell .\n",
      "walking by your love and .\n",
      "he will me love you .\n",
      ".\n",
      "was the gone .\n",
      ".\n",
      "now i can t sleep .\n",
      "to the life to hold to you right for be .\n",
      "or our need your old town .\n",
      ".\n",
      "street you .\n",
      ".\n",
      "so much out everybody s what looking as saw like hard feeling gonna it take was s bad .\n",
      "but girl what.\n",
      "113.12863636363637\n"
     ]
    }
   ],
   "source": [
    "# Example.\n",
    "s = write_song(test_model, \"a b c d there s a monkey in a\", 150, SEQUENCE_LEN, 1)\n",
    "p_s = process_output(s)\n",
    "print(p_s)\n",
    "print(score(p_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "look at her face it s a wonderful face .\n",
      "and it means something special to me .\n",
      "look at the way that she smiles when she sees me .\n",
      "how lucky can one fellow be .\n",
      ".\n",
      "she s just my kind of girl she makes me feel fine .\n",
      "who could ever believe that she could be mine .\n",
      "she s just my kind of girl without her i m blue .\n",
      "and if she ever leaves me what could i do what could i do .\n",
      ".\n",
      "and when we go for a walk in the park .\n",
      "and she holds me and squeezes my hand .\n",
      "we ll go on walking for hours and talking .\n",
      "about all the things that we plan .\n",
      ".\n",
      "she s just my kind of girl she makes me feel fine .\n",
      "who could ever believe that she could be mine .\n",
      "she s just my kind of girl without her i m blue .\n",
      "and if she ever leaves me what could i do what could i do .\n",
      ".\n",
      ".\n",
      "108.67320910973086\n"
     ]
    }
   ],
   "source": [
    "a = process_output(\" \".join(clean_songs[0]))\n",
    "print(a)\n",
    "print(score(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:  0 Mode:  5\n",
      "Running experiment 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-414ef8236f8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mprompt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msong\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mnn_song\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrite_song\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSEQUENCE_LEN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[0mnn_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_song\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mreal_song\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-882721864e5a>\u001b[0m in \u001b[0;36mwrite_song\u001b[1;34m(model, seed, song_len, word_num, diversity)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mwrite_song\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msong_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mword_num\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0msong\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0msong\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-f5ec803cc4ef>\u001b[0m in \u001b[0;36mclean\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mclean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbasic_cleaning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_punctuation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_extra_letters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-f5ec803cc4ef>\u001b[0m in \u001b[0;36mbasic_cleaning\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Needs to undergo the same clean-up, and tokenization.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbasic_cleaning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" \\n \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NUM = 5\n",
    "\n",
    "simple5_fn = [('./LocalData/Run' + str(i) + 'Simple5.h5') for i in range(3)]\n",
    "simple10_fn = [('./LocalData/Run' + str(i) + 'Simple10.h5') for i in range(10)]\n",
    "\n",
    "run = 0\n",
    "mode = 5\n",
    "test_model = load_model(('./LocalData/Run' + str(run) + 'Simple' + str(mode) + '.h5'))\n",
    "print(\"Run: \", run, \"Mode: \", mode)\n",
    "for i in range(EXPERIMENT_NUM):\n",
    "        print(\"Running experiment\", i)\n",
    "        results = np.zeros(len(clean_songs), dtype=np.float32)\n",
    "        for k in range(len(results)):\n",
    "            song = clean_songs[k]\n",
    "            prompt = song[0:5]\n",
    "\n",
    "            nn_song = write_song(test_model, prompt, len(song), SEQUENCE_LEN, 0.5)\n",
    "            nn_p = process_output(' '.join(nn_song))\n",
    "            real_song = process_output(' '.join(song))\n",
    "\n",
    "            results[k] = score(nn_p) - score(real_song)\n",
    "\n",
    "        print(\"Mean: \", np.mean(results))\n",
    "\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
