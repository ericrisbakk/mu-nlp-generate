{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation was done following this tutorial for song lyrics generation: https://medium.com/coinmonks/word-level-lstm-text-generator-creating-automatic-song-lyrics-with-neural-networks-b8a1617104fb and this tutorial for word embeddings: https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded.\n",
      "Using  0.1  of the dataset.\n",
      "Sampling finished.\n"
     ]
    }
   ],
   "source": [
    "# Load data.\n",
    "data = pd.read_csv(\"LocalData/ProcessedSongData.csv\")\n",
    "# Ensure that \"token\" and \"corrected\" columns are lists, and not strings of list.\n",
    "# When saving to csv the lists are converted into string.\n",
    "print(\"data loaded.\")\n",
    "fraction = 0.1\n",
    "print(\"Using \", fraction, \" of the dataset.\")\n",
    "data = data.sample(frac=fraction)\n",
    "data = data.reset_index(drop=True)\n",
    "print(\"Sampling finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has already been cleaned, using the script *CleanData.py*, but needs to be converted into token format again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to tokenize.\n",
      "Tokenized clean.\n",
      "Tokenized corrected.\n"
     ]
    }
   ],
   "source": [
    "# Turn sentence into list of words.\n",
    "def tokenize(s):\n",
    "    s_list = [w for w in s.split(' ') if w.strip() != '' or w == '\\r\\n']      \n",
    "    return s_list\n",
    "\n",
    "print(\"Starting to tokenize.\")\n",
    "data[\"t_clean\"] = data.clean.apply(tokenize)\n",
    "print(\"Tokenized clean.\")\n",
    "data[\"t_corrected\"] = data.corrected.apply(tokenize)\n",
    "print(\"Tokenized corrected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['close',\n",
       " 'every',\n",
       " 'door',\n",
       " 'to',\n",
       " 'me',\n",
       " '\\r\\n',\n",
       " 'hide',\n",
       " 'all',\n",
       " 'the',\n",
       " 'world',\n",
       " 'from',\n",
       " 'me',\n",
       " '\\r\\n',\n",
       " 'bar',\n",
       " 'all',\n",
       " 'the',\n",
       " 'windows',\n",
       " '\\r\\n',\n",
       " 'and',\n",
       " 'shut',\n",
       " 'out',\n",
       " 'the',\n",
       " 'light',\n",
       " '\\r\\n',\n",
       " 'do',\n",
       " 'what',\n",
       " 'you',\n",
       " 'want',\n",
       " 'with',\n",
       " 'me',\n",
       " '\\r\\n',\n",
       " 'hate',\n",
       " 'me',\n",
       " 'and',\n",
       " 'laugh',\n",
       " 'at',\n",
       " 'me',\n",
       " '\\r\\n',\n",
       " 'darken',\n",
       " 'my',\n",
       " 'daytime',\n",
       " '\\r\\n',\n",
       " 'and',\n",
       " 'torture',\n",
       " 'my',\n",
       " 'night',\n",
       " '\\r\\n',\n",
       " '\\r\\n',\n",
       " 'if',\n",
       " 'my',\n",
       " 'life',\n",
       " 'were',\n",
       " 'important',\n",
       " 'i',\n",
       " '\\r\\n',\n",
       " 'would',\n",
       " 'ask',\n",
       " 'will',\n",
       " 'i',\n",
       " 'live',\n",
       " 'or',\n",
       " 'die',\n",
       " '\\r\\n',\n",
       " 'but',\n",
       " 'i',\n",
       " 'know',\n",
       " 'the',\n",
       " 'answers',\n",
       " 'lie',\n",
       " '\\r\\n',\n",
       " 'far',\n",
       " 'from',\n",
       " 'this',\n",
       " 'world',\n",
       " '\\r\\n',\n",
       " '\\r\\n',\n",
       " 'just',\n",
       " 'give',\n",
       " 'me',\n",
       " 'a',\n",
       " 'number',\n",
       " '\\r\\n',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'my',\n",
       " 'name',\n",
       " '\\r\\n',\n",
       " 'forget',\n",
       " 'all',\n",
       " 'about',\n",
       " 'me',\n",
       " '\\r\\n',\n",
       " 'and',\n",
       " 'let',\n",
       " 'me',\n",
       " 'decay',\n",
       " '\\r\\n',\n",
       " 'i',\n",
       " 'do',\n",
       " 'not',\n",
       " 'matter',\n",
       " '\\r\\n',\n",
       " 'i',\n",
       " 'm',\n",
       " 'only',\n",
       " 'one',\n",
       " 'person',\n",
       " '\\r\\n',\n",
       " 'destroy',\n",
       " 'me',\n",
       " 'completely',\n",
       " '\\r\\n',\n",
       " 'then',\n",
       " 'throw',\n",
       " 'me',\n",
       " 'away',\n",
       " '\\r\\n',\n",
       " '\\r\\n',\n",
       " 'if',\n",
       " 'my',\n",
       " 'life',\n",
       " 'were',\n",
       " 'important',\n",
       " 'i',\n",
       " '\\r\\n',\n",
       " 'would',\n",
       " 'ask',\n",
       " 'will',\n",
       " 'i',\n",
       " 'live',\n",
       " 'or',\n",
       " 'die',\n",
       " '\\r\\n',\n",
       " 'but',\n",
       " 'i',\n",
       " 'know',\n",
       " 'the',\n",
       " 'answers',\n",
       " 'lie',\n",
       " '\\r\\n',\n",
       " 'far',\n",
       " 'from',\n",
       " 'this',\n",
       " 'world',\n",
       " '\\r\\n',\n",
       " '\\r\\n',\n",
       " 'close',\n",
       " 'every',\n",
       " 'door',\n",
       " 'to',\n",
       " 'me',\n",
       " '\\r\\n',\n",
       " 'keep',\n",
       " 'those',\n",
       " 'i',\n",
       " 'love',\n",
       " 'from',\n",
       " 'me',\n",
       " '\\r\\n',\n",
       " 'children',\n",
       " 'of',\n",
       " 'israel',\n",
       " '\\r\\n',\n",
       " 'are',\n",
       " 'never',\n",
       " 'alone',\n",
       " '\\r\\n',\n",
       " '\\r\\n',\n",
       " 'for',\n",
       " 'we',\n",
       " 'know',\n",
       " 'we',\n",
       " 'shall',\n",
       " 'find',\n",
       " '\\r\\n',\n",
       " 'our',\n",
       " 'own',\n",
       " 'peace',\n",
       " 'of',\n",
       " 'mind',\n",
       " '\\r\\n',\n",
       " 'for',\n",
       " 'i',\n",
       " 'have',\n",
       " 'been',\n",
       " 'promised',\n",
       " '\\r\\n',\n",
       " 'a',\n",
       " 'land',\n",
       " 'of',\n",
       " '\\r\\n',\n",
       " 'a',\n",
       " 'land',\n",
       " 'of',\n",
       " 'our',\n",
       " 'own',\n",
       " '\\r\\n',\n",
       " '\\r\\n',\n",
       " 'if',\n",
       " 'my',\n",
       " 'life',\n",
       " 'were',\n",
       " 'important',\n",
       " 'i',\n",
       " '\\r\\n',\n",
       " 'would',\n",
       " 'ask',\n",
       " 'will',\n",
       " 'i',\n",
       " 'live',\n",
       " 'or',\n",
       " 'die',\n",
       " '\\r\\n',\n",
       " 'but',\n",
       " 'i',\n",
       " 'know',\n",
       " 'the',\n",
       " 'answers',\n",
       " 'lie',\n",
       " '\\r\\n',\n",
       " 'far',\n",
       " 'from',\n",
       " 'this',\n",
       " 'world',\n",
       " '\\r\\n',\n",
       " '\\r\\n',\n",
       " 'close',\n",
       " 'every',\n",
       " 'door',\n",
       " '\\r\\n',\n",
       " 'close',\n",
       " 'every',\n",
       " 'door',\n",
       " 'to',\n",
       " 'me',\n",
       " '\\r\\n',\n",
       " '\\r\\n']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm output looks correct.\n",
    "data.t_corrected[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words total:  1584642\n",
      "Unique words:  22415\n"
     ]
    }
   ],
   "source": [
    "text_values = data.t_corrected.values\n",
    "vocab = Counter()\n",
    "\n",
    "text_in_words = []\n",
    "for song in text_values:\n",
    "    vocab.update(song)\n",
    "    text_in_words.extend(song)\n",
    "\n",
    "print(\"Number of words total: \", len(text_in_words))\n",
    "print(\"Unique words: \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words before ignoring: 22415\n",
      "Ignoring words with frequency < 2\n",
      "Unique words after ignoring: 13981\n"
     ]
    }
   ],
   "source": [
    "# Calculate word frequency\n",
    "# With a minimum word frequency of 2, all words that only\n",
    "# ever appear once will be ignored.\n",
    "MIN_WORD_FREQUENCY=2\n",
    "\n",
    "ignored_words = set()\n",
    "for k, v in vocab.items():\n",
    "    if vocab[k] < MIN_WORD_FREQUENCY:\n",
    "        ignored_words.add(k)\n",
    "\n",
    "print('Unique words before ignoring:', len(vocab))\n",
    "print('Ignoring words with frequency <', MIN_WORD_FREQUENCY)\n",
    "words_reduced = sorted(set(vocab.keys()) - ignored_words)\n",
    "print('Unique words after ignoring:', len(words_reduced))\n",
    "\n",
    "word_indices = dict((c, i) for i, c in enumerate(words_reduced))\n",
    "indices_word = dict((i, c) for i, c in enumerate(words_reduced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the vocabulary to a file.\n",
    "def save_list(lines, filename):\n",
    "\t# convert lines to a single blob of text\n",
    "\tdata = '\\n'.join(lines)\n",
    "\t# open file\n",
    "\tfile = open(filename, 'w')\n",
    "\t# write text\n",
    "\tfile.write(data)\n",
    "\t# close file\n",
    "\tfile.close()\n",
    "    \n",
    "# save tokens to a vocabulary file\n",
    "save_list(words_reduced, 'LocalData/vocab_min2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean out words that are not in vocab, turn back into strings.\n",
    "#clean_songs = []\n",
    "#c = 0\n",
    "#for song in text_values:\n",
    "#    c += 1\n",
    "#    if c%100 == 0:\n",
    "#        print(c)\n",
    "#    clean_songs.append([w for w in song if w in words_reduced])\n",
    "\n",
    "# Because of a slow computer, I'm skipping this step, and using a fraction of the dataset, \n",
    "# Only using a fraction of the words.\n",
    "clean_songs = text_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting tokenizer on docs\n",
      "Tokenizer fitted. Encoding songs.\n",
      "Songs encoded. Padding sequences.\n"
     ]
    }
   ],
   "source": [
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "print(\"Fitting tokenizer on docs\")\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(clean_songs)\n",
    "print(\"Tokenizer fitted. Encoding songs.\")\n",
    "# sequence encode\n",
    "encoded_songs = tokenizer.texts_to_sequences(clean_songs)\n",
    "print(\"Songs encoded. Padding sequences.\")\n",
    "max_length = max([len(s) for s in clean_songs])\n",
    "songs = pad_sequences(encoded_songs, maxlen=max_length, padding='post')\n",
    "print(\"Songs padded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5765, 929)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model!\n",
    "\n",
    "EMBEDDING_SIZE = 100\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(clean_songs), EMBEDDING_SIZE, input_length=max_length))\n",
    "model.add(Bidirectional(LSTM(128), input_shape=(EMBEDDING_SIZE), len(vocab)))) \n",
    "model.add(Dropout(DROPOUT))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
