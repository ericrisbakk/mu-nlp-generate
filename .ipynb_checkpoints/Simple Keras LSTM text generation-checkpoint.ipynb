{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation was done following this tutorial for song lyrics generation: https://medium.com/coinmonks/word-level-lstm-text-generator-creating-automatic-song-lyrics-with-neural-networks-b8a1617104fb,\n",
    "a continuation of the previous tutorial using word embeddings: https://medium.com/@enriqueav/update-automatic-song-lyrics-creator-with-word-embeddings-e30de94db8d1, and this tutorial for word embeddings: https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded.\n",
      "Using  1  of the dataset.\n",
      "Sampling finished.\n"
     ]
    }
   ],
   "source": [
    "# Load data.\n",
    "data = pd.read_csv(\"LocalData/ProcessedSongData.csv\")\n",
    "# Ensure that \"token\" and \"corrected\" columns are lists, and not strings of list.\n",
    "# When saving to csv the lists are converted into string.\n",
    "print(\"data loaded.\")\n",
    "fraction = 1\n",
    "print(\"Using \", fraction, \" of the dataset.\")\n",
    "data = data.sample(frac=fraction)\n",
    "data = data.reset_index(drop=True)\n",
    "print(\"Sampling finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "The data has already been cleaned, using the script *CleanData.py*, but needs to be converted into token format again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to tokenize.\n",
      "Tokenized clean.\n",
      "Tokenized corrected.\n"
     ]
    }
   ],
   "source": [
    "# Turn sentence into list of words.\n",
    "def tokenize(s):\n",
    "    s_list = [w for w in s.split(' ') if w.strip() != '' or w == '\\r\\n']      \n",
    "    return s_list\n",
    "\n",
    "print(\"Starting to tokenize.\")\n",
    "data[\"t_clean\"] = data.clean.apply(tokenize)\n",
    "print(\"Tokenized clean.\")\n",
    "data[\"t_corrected\"] = data.corrected.apply(tokenize)\n",
    "print(\"Tokenized corrected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['close',\n",
       " 'every',\n",
       " 'door',\n",
       " 'to',\n",
       " 'me',\n",
       " '\\r\\n',\n",
       " 'hide',\n",
       " 'all',\n",
       " 'the',\n",
       " 'world',\n",
       " 'from',\n",
       " 'me',\n",
       " '\\r\\n',\n",
       " 'bar',\n",
       " 'all',\n",
       " 'the',\n",
       " 'windows',\n",
       " '\\r\\n',\n",
       " 'and',\n",
       " 'shut',\n",
       " 'out',\n",
       " 'the',\n",
       " 'light',\n",
       " '\\r\\n',\n",
       " 'do',\n",
       " 'what',\n",
       " 'you',\n",
       " 'want',\n",
       " 'with',\n",
       " 'me',\n",
       " '\\r\\n',\n",
       " 'hate',\n",
       " 'me',\n",
       " 'and',\n",
       " 'laugh',\n",
       " 'at',\n",
       " 'me',\n",
       " '\\r\\n',\n",
       " 'darken',\n",
       " 'my',\n",
       " 'daytime',\n",
       " '\\r\\n',\n",
       " 'and',\n",
       " 'torture',\n",
       " 'my',\n",
       " 'night',\n",
       " '\\r\\n',\n",
       " '\\r\\n',\n",
       " 'if',\n",
       " 'my',\n",
       " 'life',\n",
       " 'were',\n",
       " 'important',\n",
       " 'i',\n",
       " '\\r\\n',\n",
       " 'would',\n",
       " 'ask',\n",
       " 'will',\n",
       " 'i',\n",
       " 'live',\n",
       " 'or',\n",
       " 'die',\n",
       " '\\r\\n',\n",
       " 'but',\n",
       " 'i',\n",
       " 'know',\n",
       " 'the',\n",
       " 'answers',\n",
       " 'lie',\n",
       " '\\r\\n',\n",
       " 'far',\n",
       " 'from',\n",
       " 'this',\n",
       " 'world',\n",
       " '\\r\\n',\n",
       " '\\r\\n',\n",
       " 'just',\n",
       " 'give',\n",
       " 'me',\n",
       " 'a',\n",
       " 'number',\n",
       " '\\r\\n',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'my',\n",
       " 'name',\n",
       " '\\r\\n',\n",
       " 'forget',\n",
       " 'all',\n",
       " 'about',\n",
       " 'me',\n",
       " '\\r\\n',\n",
       " 'and',\n",
       " 'let',\n",
       " 'me',\n",
       " 'decay',\n",
       " '\\r\\n',\n",
       " 'i',\n",
       " 'do',\n",
       " 'not',\n",
       " 'matter',\n",
       " '\\r\\n',\n",
       " 'i',\n",
       " 'm',\n",
       " 'only',\n",
       " 'one',\n",
       " 'person',\n",
       " '\\r\\n',\n",
       " 'destroy',\n",
       " 'me',\n",
       " 'completely',\n",
       " '\\r\\n',\n",
       " 'then',\n",
       " 'throw',\n",
       " 'me',\n",
       " 'away',\n",
       " '\\r\\n',\n",
       " '\\r\\n',\n",
       " 'if',\n",
       " 'my',\n",
       " 'life',\n",
       " 'were',\n",
       " 'important',\n",
       " 'i',\n",
       " '\\r\\n',\n",
       " 'would',\n",
       " 'ask',\n",
       " 'will',\n",
       " 'i',\n",
       " 'live',\n",
       " 'or',\n",
       " 'die',\n",
       " '\\r\\n',\n",
       " 'but',\n",
       " 'i',\n",
       " 'know',\n",
       " 'the',\n",
       " 'answers',\n",
       " 'lie',\n",
       " '\\r\\n',\n",
       " 'far',\n",
       " 'from',\n",
       " 'this',\n",
       " 'world',\n",
       " '\\r\\n',\n",
       " '\\r\\n',\n",
       " 'close',\n",
       " 'every',\n",
       " 'door',\n",
       " 'to',\n",
       " 'me',\n",
       " '\\r\\n',\n",
       " 'keep',\n",
       " 'those',\n",
       " 'i',\n",
       " 'love',\n",
       " 'from',\n",
       " 'me',\n",
       " '\\r\\n',\n",
       " 'children',\n",
       " 'of',\n",
       " 'israel',\n",
       " '\\r\\n',\n",
       " 'are',\n",
       " 'never',\n",
       " 'alone',\n",
       " '\\r\\n',\n",
       " '\\r\\n',\n",
       " 'for',\n",
       " 'we',\n",
       " 'know',\n",
       " 'we',\n",
       " 'shall',\n",
       " 'find',\n",
       " '\\r\\n',\n",
       " 'our',\n",
       " 'own',\n",
       " 'peace',\n",
       " 'of',\n",
       " 'mind',\n",
       " '\\r\\n',\n",
       " 'for',\n",
       " 'i',\n",
       " 'have',\n",
       " 'been',\n",
       " 'promised',\n",
       " '\\r\\n',\n",
       " 'a',\n",
       " 'land',\n",
       " 'of',\n",
       " '\\r\\n',\n",
       " 'a',\n",
       " 'land',\n",
       " 'of',\n",
       " 'our',\n",
       " 'own',\n",
       " '\\r\\n',\n",
       " '\\r\\n',\n",
       " 'if',\n",
       " 'my',\n",
       " 'life',\n",
       " 'were',\n",
       " 'important',\n",
       " 'i',\n",
       " '\\r\\n',\n",
       " 'would',\n",
       " 'ask',\n",
       " 'will',\n",
       " 'i',\n",
       " 'live',\n",
       " 'or',\n",
       " 'die',\n",
       " '\\r\\n',\n",
       " 'but',\n",
       " 'i',\n",
       " 'know',\n",
       " 'the',\n",
       " 'answers',\n",
       " 'lie',\n",
       " '\\r\\n',\n",
       " 'far',\n",
       " 'from',\n",
       " 'this',\n",
       " 'world',\n",
       " '\\r\\n',\n",
       " '\\r\\n',\n",
       " 'close',\n",
       " 'every',\n",
       " 'door',\n",
       " '\\r\\n',\n",
       " 'close',\n",
       " 'every',\n",
       " 'door',\n",
       " 'to',\n",
       " 'me',\n",
       " '\\r\\n',\n",
       " '\\r\\n']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm output looks correct.\n",
    "data.t_corrected[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words total:  1584642\n",
      "Unique words:  22415\n"
     ]
    }
   ],
   "source": [
    "text_values = data.t_corrected.values\n",
    "vocab = Counter()\n",
    "\n",
    "text_in_words = []\n",
    "for song in text_values:\n",
    "    vocab.update(song)\n",
    "    text_in_words.extend(song)\n",
    "\n",
    "print(\"Number of words total: \", len(text_in_words))\n",
    "print(\"Unique words: \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words before ignoring: 22415\n",
      "Ignoring words with frequency < 2\n",
      "Unique words after ignoring: 13981\n"
     ]
    }
   ],
   "source": [
    "# Calculate word frequency\n",
    "# With a minimum word frequency of 2, all words that only\n",
    "# ever appear once will be ignored.\n",
    "MIN_WORD_FREQUENCY=2\n",
    "\n",
    "ignored_words = set()\n",
    "for k, v in vocab.items():\n",
    "    if vocab[k] < MIN_WORD_FREQUENCY:\n",
    "        ignored_words.add(k)\n",
    "\n",
    "print('Unique words before ignoring:', len(vocab))\n",
    "print('Ignoring words with frequency <', MIN_WORD_FREQUENCY)\n",
    "words_reduced = sorted(set(vocab.keys()) - ignored_words)\n",
    "print('Unique words after ignoring:', len(words_reduced))\n",
    "\n",
    "\n",
    "#word_indices = dict((c, i) for i, c in enumerate(words_reduced))\n",
    "#indices_word = dict((i, c) for i, c in enumerate(words_reduced))\n",
    "\n",
    "# Because we are not using the reduced vocabulary, do this instead.\n",
    "word_indices = dict((c, i) for i, c in enumerate(vocab))\n",
    "indices_word = dict((i, c) for i, c in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the vocabulary to a file.\n",
    "def save_list(lines, filename):\n",
    "\t# convert lines to a single blob of text\n",
    "\tdata = '\\n'.join(lines)\n",
    "\t# open file\n",
    "\tfile = open(filename, 'w')\n",
    "\t# write text\n",
    "\tfile.write(data)\n",
    "\t# close file\n",
    "\tfile.close()\n",
    "    \n",
    "# save tokens to a vocabulary file\n",
    "save_list(words_reduced, 'LocalData/vocab_min2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean out words that are not in vocab, turn back into strings.\n",
    "#clean_songs = []\n",
    "#c = 0\n",
    "#for song in text_values:\n",
    "#    c += 1\n",
    "#    if c%100 == 0:\n",
    "#        print(c)\n",
    "#    clean_songs.append([w for w in song if w in words_reduced])\n",
    "\n",
    "# Because of a slow computer, I'm skipping this step, and using a fraction of the dataset, \n",
    "# Only using a fraction of the words.\n",
    "clean_songs = text_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Word embedding\n",
    "\n",
    "Below we use *gensim* to train a custom word embedding on our song dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "EMBEDDING_SIZE = 100\n",
    "WINDOW_SIZE = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model and keyed vectors, so training does not have to happen again.\n",
    "wv_mod = Word2Vec(clean_songs, size=EMBEDDING_SIZE, window=WINDOW_SIZE, min_count=1)\n",
    "wv_mod.save(\"LocalData/song_word2vec.model\")\n",
    "wv_mod.wv.save(\"LocalData/song_word_vec.kv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = wv_mod.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('best', 1.0),\n",
       " ('friend', 0.6336452960968018),\n",
       " ('loving', 0.6079089641571045),\n",
       " ('because', 0.600318193435669),\n",
       " ('things', 0.549144983291626)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = [wv['best']]\n",
    "wv.most_similar(positive=pos, topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Dataset into X and Ys\n",
    "The data is still a list of strings. We need to be able to convert our input to word vectors, and our output needs to be a one-hot-encoding, as the prediction can be interpreted as a sort of classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "SEQUENCE_LEN = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(clean_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3938]]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Activation, Bidirectional, LSTM, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Songs encoded. Padding sequences.\n"
     ]
    }
   ],
   "source": [
    "max_length = max([len(s) for s in clean_songs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model!\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model = Sequential()\n",
    "# We will convert any text to a word embedding before sending it on its way.\n",
    "model.add(Bidirectional(LSTM(128), input_shape=(SEQUENCE_LEN, EMBEDDING_SIZE) ) )\n",
    "model.add(Dropout(DROPOUT))\n",
    "# Classification of next word.\n",
    "model.add(Dense(len(vocab))) \n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate batch to train on based on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator.\n",
    "def generator(sentence_list, next_word_list, batch_size):\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, SEQUENCE_LEN, len(words)), dtype=np.bool)\n",
    "        y = np.zeros((batch_size, len(words)), dtype=np.bool)\n",
    "        for i in range(batch_size):\n",
    "            for t, w in enumerate(sentence_list[index]):\n",
    "                x[i, t, word_indices[w]] = 1\n",
    "            y[i, word_indices[next_word_list[index]]] = 1\n",
    "\n",
    "            index = index + 1\n",
    "            if index == len(sentence_list):\n",
    "                index = 0\n",
    "        yield x, y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
