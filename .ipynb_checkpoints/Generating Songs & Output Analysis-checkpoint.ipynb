{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "## 1. Generating Songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from collections import Counter\n",
    "from keras.models import load_model\n",
    "from gensim.models import KeyedVectors\n",
    "from textatistic import Textatistic\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "data loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "print(\"Loading data\")\n",
    "data = pd.read_csv(\"./LocalData/ProcessedSongData.csv\")\n",
    "print(\"data loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating Songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for preparing user input.\n",
    "# Needs to undergo the same clean-up, and tokenization.\n",
    "def basic_cleaning(sentence):\n",
    "    s = sentence.lower()\n",
    "    s = s.replace(\"\\n\", \" \\n \")\n",
    "    return s\n",
    "\n",
    "def tokenize(s):\n",
    "    s_list = [w for w in s.split(' ') if w.strip() != '' or w == '\\r\\n']\n",
    "    for i, w in enumerate(s_list):\n",
    "        if w == '\\r\\n':\n",
    "            s_list[i] = '\\\\r\\\\n'\n",
    "    return s_list\n",
    "\n",
    "# Let us clean the token list with this new information.\n",
    "# The below removes anything except whitespace and alphanumeric characters.\n",
    "def remove_punctuation(s):\n",
    "    return re.sub('[^\\w\\s]', ' ', s)\n",
    "\n",
    "# There are, to my awareness, no words with consecutive \n",
    "# three same letters in english.\n",
    "def remove_extra_letters(s):\n",
    "    return re.sub(r\"(.)\\1{2,}\", r\"\\1\"*2, s)\n",
    "\n",
    "def clean(sentence):\n",
    "    s = basic_cleaning(sentence)\n",
    "    s = remove_punctuation(s)\n",
    "    s = remove_extra_letters(s)\n",
    "    return tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['t_corrected'] = data['corrected'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocab.\n",
      "Number of words total:  15749211\n",
      "Unique words:  61999\n"
     ]
    }
   ],
   "source": [
    "# Prep vocab\n",
    "print(\"Creating vocab.\")\n",
    "text_values = data.t_corrected.values\n",
    "vocab = Counter()\n",
    "\n",
    "text_in_words = []\n",
    "for song in text_values:\n",
    "    vocab.update(song)\n",
    "    text_in_words.extend(song)\n",
    "\n",
    "print(\"Number of words total: \", len(text_in_words))\n",
    "print(\"Unique words: \", len(vocab))\n",
    "\n",
    "vocab_keys = sorted(list(vocab.keys()))\n",
    "\n",
    "clean_songs = text_values\n",
    "\n",
    "word_indices = dict((c, i) for i, c in enumerate(vocab_keys))\n",
    "indices_word = dict((i, c) for i, c in enumerate(vocab_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "MODEL_FILE = './LocalData/Final_20190530170302Type_256_256.h5'\n",
    "SEQUENCE_LEN= 10\n",
    "test_model = load_model(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Keyed Vectors.\n"
     ]
    }
   ],
   "source": [
    "# Load Keyed Vectors\n",
    "print(\"Loading Keyed Vectors.\")\n",
    "EMBEDDING_SIZE = 100\n",
    "wv = KeyedVectors.load(\"./LocalData/song_word_vec.kv\")\n",
    "\n",
    "wv['\\\\r\\\\n'] = wv['\\r\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore this for now.\n",
    "def generate_nn_data(sentence_list, next_word_list):\n",
    "    x = np.zeros((len(sentence_list), SEQUENCE_LEN, EMBEDDING_SIZE), dtype=np.float32)\n",
    "    y = np.zeros(len(next_word_list), dtype=np.int32)\n",
    "    # Go through each sentence fragment\n",
    "    for i, s in enumerate(sentence_list):\n",
    "        # For each word in the sentence fragment, get the vector\n",
    "        for t, w in enumerate(s):\n",
    "            # If word not recognized, leave blank.\n",
    "            if w in wv:\n",
    "                x[i, t, :] = wv[w]\n",
    "            else:\n",
    "                print(\"Word unrecognized: \", w)\n",
    "                \n",
    "        # Set the appropriate y-value.\n",
    "        y[i] = word_indices[next_word_list[i]]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The character '\\\\\\\\r\\\\\\\\n' is very often predicted as the most likely value - except in cases such as \"i'm\", in which case the 2-layer model usually ends in the correct way.\n",
    "\n",
    "To offset this, it might be interesting to penalize the '\\\\\\\\r\\\\\\\\n' depending on how long it was since we last saw it. Such functionality has been added below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction method\n",
    "# Functions from keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "# Only select from the *num* highest predictions.\n",
    "def sample_n(preds, n, diversity):\n",
    "    \n",
    "    indices = sorted(range(len(preds)), key=lambda i: preds[i])[-n:]\n",
    "    index_vals = []\n",
    "    word_vals = []\n",
    "    for index in indices:\n",
    "        index_vals.append(preds[index])\n",
    "        word_vals.append(indices_word[index])\n",
    "    \n",
    "    index = sample(index_vals, diversity)\n",
    "    \n",
    "    # See output.\n",
    "    if False:\n",
    "        print(\"\\nSample N.\")\n",
    "        print(\"Indices: \", indices)\n",
    "        print(\"Index vals: \", index_vals)\n",
    "        print(\"words: \", word_vals)\n",
    "        print(\"Chosen: \", indices[index])\n",
    "    \n",
    "    return indices[index]\n",
    "\n",
    "# Counts how many steps back since\n",
    "# newline character from end of string list\n",
    "def words_since_newline(s_list):\n",
    "    for i in range(len(s_list)):\n",
    "        if s_list[-(i+1)] == '\\\\r\\\\n':\n",
    "            return i\n",
    "    return len(s_list)\n",
    "\n",
    "        \n",
    "def penalize_newline(preds, words_since_newline, newline_limit, p_weight, max_1=False):\n",
    "    p_i = word_indices['\\\\r\\\\n']\n",
    "    p_factor = p_weight*(words_since_newline/newline_limit)\n",
    "    if max_1 and p_factor > 1:\n",
    "        p_factor = 1\n",
    "    preds[p_i] = preds[p_i]*p_factor\n",
    "    \n",
    "    \n",
    "# Given *seed*, predicts the next word.\n",
    "def predict_word(model, seed, word_num, diversity, n_largest=-1, newline_limit = -1, p_weight=1):\n",
    "    # clean up the sentence.\n",
    "    if type(seed) == str:\n",
    "        s = clean(seed)[-word_num:]\n",
    "    else:\n",
    "        s = seed\n",
    "    # Give user warning if some word is not recognized.\n",
    "    for i, w in enumerate(s):\n",
    "        if w not in wv:\n",
    "            print(\"WARNING: The word \", w, \" is not in this vocabulary.\")\n",
    "    \n",
    "    x_pred = np.zeros((1, SEQUENCE_LEN, EMBEDDING_SIZE), dtype=np.float32)\n",
    "    for t, w in enumerate(s):\n",
    "        x_pred[0, t, :] = wv[w]    \n",
    "    \n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    \n",
    "    # Penalize newline?\n",
    "    if not (newline_limit < 0 or p_weight < 0):\n",
    "        # print(\"Penalizing newline!\")\n",
    "        penalize_newline(preds, words_since_newline(s), newline_limit, p_weight)\n",
    "    \n",
    "    # Extract index of next word.\n",
    "    if n_largest <= 0:\n",
    "        next_index = sample(preds, diversity)\n",
    "    else:\n",
    "        next_index = sample_n(preds, n_largest, diversity)\n",
    "    # Get that word\n",
    "    next_word = indices_word[next_index]\n",
    "\n",
    "    return next_word\n",
    "\n",
    "\n",
    "# Calls sample repeatedly to create a song with *word_num* words in it, returns a string.\n",
    "def write_song(model, seed, song_len, word_num, diversity, n_largest=-1, newline_limit=-1, p_weight=1):\n",
    "    s = clean(seed)[-word_num:]\n",
    "    song = []\n",
    "    song.extend(s)\n",
    "    for i in range(song_len):\n",
    "        pred = predict_word(model, s, word_num, diversity, \n",
    "                            n_largest=n_largest, newline_limit=newline_limit, p_weight=p_weight)\n",
    "        song.append(pred)\n",
    "        s = s[1:]\n",
    "        s.append(pred)\n",
    "    \n",
    "    return \" \".join(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\heier\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'death death devil devil devil devil evil evil songs yeah \\\\r\\\\n you re a little one \\\\r\\\\n the night i want for the love \\\\r\\\\n i was the in my heart \\\\r\\\\n you can t \\\\r\\\\n and i don t know i don t \\\\r\\\\n and i know i ve been \\\\r\\\\n it s gonna be so i know \\\\r\\\\n'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's run some examples!\n",
    "write_song(test_model, \"death death devil devil devil devil evil evil songs yeah\", 50, SEQUENCE_LEN, \n",
    "           0.8, n_largest = 10, newline_limit=12, p_weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = re.compile('(\\s){2,}')\n",
    "\n",
    "def process_output(song):\n",
    "    song = song.replace('\\\\r\\\\n', '.\\n')\n",
    "    song = pat.sub('\\n', song)\n",
    "    if song[-1] != '.':\n",
    "        song += '.'\n",
    "    return song\n",
    "\n",
    "# Scoring method!\n",
    "def score(lyrics):\n",
    "    s = Textatistic(lyrics)\n",
    "    return s.flesch_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a b c d there s a monkey in a move time .\n",
      "it we went to play but a .\n",
      "to your face .\n",
      "i said i time .\n",
      ".\n",
      "that i m fine .\n",
      "and and my .\n",
      ".\n",
      "to to want you .\n",
      "but you people say once for you .\n",
      "a long day .\n",
      "we say and yeah and the in .\n",
      "some and tell .\n",
      "and we wanna t some way started for who fight tonight while .\n",
      "leave nobody at girl people .\n",
      "but i .\n",
      "i stay .\n",
      "she s .\n",
      ".\n",
      "it .\n",
      "i in the wind and there and it we wish but .\n",
      ".\n",
      "i a time .\n",
      "or in this window .\n",
      ".\n",
      "the rain on a place on all to .\n",
      "my sweet re your world it s a long night leave one feeling why day your .\n",
      "i m the one i need to just never touch.\n",
      "113.59720864661655\n"
     ]
    }
   ],
   "source": [
    "# Example.\n",
    "s = write_song(test_model, \"a b c d there s a monkey in a\", 150, SEQUENCE_LEN, 1)\n",
    "p_s = process_output(s)\n",
    "print(p_s)\n",
    "print(score(p_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "look at her face it s a wonderful face .\n",
      "and it means something special to me .\n",
      "look at the way that she smiles when she sees me .\n",
      "how lucky can one fellow be .\n",
      ".\n",
      "she s just my kind of girl she makes me feel fine .\n",
      "who could ever believe that she could be mine .\n",
      "she s just my kind of girl without her i m blue .\n",
      "and if she ever leaves me what could i do what could i do .\n",
      ".\n",
      "and when we go for a walk in the park .\n",
      "and she holds me and squeezes my hand .\n",
      "we ll go on walking for hours and talking .\n",
      "about all the things that we plan .\n",
      ".\n",
      "she s just my kind of girl she makes me feel fine .\n",
      "who could ever believe that she could be mine .\n",
      "she s just my kind of girl without her i m blue .\n",
      "and if she ever leaves me what could i do what could i do .\n",
      ".\n",
      ".\n",
      "108.67320910973086\n"
     ]
    }
   ],
   "source": [
    "a = process_output(\" \".join(clean_songs[0]))\n",
    "print(a)\n",
    "print(score(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:  0 Mode:  5\n",
      "Running experiment 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-cf5f7d961075>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mprompt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msong\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mnn_song\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrite_song\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSEQUENCE_LEN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[0mnn_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_song\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mreal_song\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-13e4033c767a>\u001b[0m in \u001b[0;36mwrite_song\u001b[1;34m(model, seed, song_len, word_num, diversity)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mwrite_song\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msong_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mword_num\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0msong\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0msong\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-f5ec803cc4ef>\u001b[0m in \u001b[0;36mclean\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mclean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbasic_cleaning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_punctuation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_extra_letters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-f5ec803cc4ef>\u001b[0m in \u001b[0;36mbasic_cleaning\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Needs to undergo the same clean-up, and tokenization.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbasic_cleaning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" \\n \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NUM = 5\n",
    "\n",
    "simple5_fn = [('./LocalData/Run' + str(i) + 'Simple5.h5') for i in range(3)]\n",
    "simple10_fn = [('./LocalData/Run' + str(i) + 'Simple10.h5') for i in range(10)]\n",
    "\n",
    "run = 0\n",
    "mode = 5\n",
    "test_model = load_model(('./LocalData/Run' + str(run) + 'Simple' + str(mode) + '.h5'))\n",
    "print(\"Run: \", run, \"Mode: \", mode)\n",
    "for i in range(EXPERIMENT_NUM):\n",
    "        print(\"Running experiment\", i)\n",
    "        results = np.zeros(len(clean_songs), dtype=np.float32)\n",
    "        for k in range(len(results)):\n",
    "            song = clean_songs[k]\n",
    "            prompt = song[0:5]\n",
    "\n",
    "            nn_song = write_song(test_model, prompt, len(song), SEQUENCE_LEN, 0.5)\n",
    "            nn_p = process_output(' '.join(nn_song))\n",
    "            real_song = process_output(' '.join(song))\n",
    "\n",
    "            results[k] = score(nn_p) - score(real_song)\n",
    "\n",
    "        print(\"Mean: \", np.mean(results))\n",
    "\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
