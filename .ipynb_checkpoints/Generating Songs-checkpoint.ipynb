{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "\n",
    "This notebook showcases how to generate outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from collections import Counter\n",
    "from keras.models import load_model\n",
    "from gensim.models import KeyedVectors\n",
    "from textatistic import Textatistic\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "data loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "print(\"Loading data\")\n",
    "data = pd.read_csv(\"./LocalData/ProcessedSongData.csv\")\n",
    "print(\"data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for preparing user input.\n",
    "# Needs to undergo the same clean-up, and tokenization.\n",
    "def basic_cleaning(sentence):\n",
    "    s = sentence.lower()\n",
    "    s = s.replace(\"\\n\", \" \\n \")\n",
    "    return s\n",
    "\n",
    "def tokenize(s):\n",
    "    s_list = [w for w in s.split(' ') if w.strip() != '' or w == '\\r\\n']\n",
    "    for i, w in enumerate(s_list):\n",
    "        if w == '\\r\\n':\n",
    "            s_list[i] = '\\\\r\\\\n'\n",
    "    return s_list\n",
    "\n",
    "# Let us clean the token list with this new information.\n",
    "# The below removes anything except whitespace and alphanumeric characters.\n",
    "def remove_punctuation(s):\n",
    "    return re.sub('[^\\w\\s]', ' ', s)\n",
    "\n",
    "# There are, to my awareness, no words with consecutive \n",
    "# three same letters in english.\n",
    "def remove_extra_letters(s):\n",
    "    return re.sub(r\"(.)\\1{2,}\", r\"\\1\"*2, s)\n",
    "\n",
    "def clean(sentence):\n",
    "    s = basic_cleaning(sentence)\n",
    "    s = remove_punctuation(s)\n",
    "    s = remove_extra_letters(s)\n",
    "    return tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['t_corrected'] = data['corrected'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocab.\n",
      "Number of words total:  15749211\n",
      "Unique words:  61999\n"
     ]
    }
   ],
   "source": [
    "# Prep vocab\n",
    "print(\"Creating vocab.\")\n",
    "text_values = data.t_corrected.values\n",
    "vocab = Counter()\n",
    "\n",
    "text_in_words = []\n",
    "for song in text_values:\n",
    "    vocab.update(song)\n",
    "    text_in_words.extend(song)\n",
    "\n",
    "print(\"Number of words total: \", len(text_in_words))\n",
    "print(\"Unique words: \", len(vocab))\n",
    "\n",
    "vocab_keys = sorted(list(vocab.keys()))\n",
    "\n",
    "clean_songs = text_values\n",
    "\n",
    "word_indices = dict((c, i) for i, c in enumerate(vocab_keys))\n",
    "indices_word = dict((i, c) for i, c in enumerate(vocab_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "MODEL_FILE = './LocalData/Final_20190529124136Type10.h5'\n",
    "SEQUENCE_LEN= 10\n",
    "test_model = load_model(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Keyed Vectors.\n"
     ]
    }
   ],
   "source": [
    "# Load Keyed Vectors\n",
    "print(\"Loading Keyed Vectors.\")\n",
    "EMBEDDING_SIZE = 100\n",
    "wv = KeyedVectors.load(\"./LocalData/song_word_vec.kv\")\n",
    "\n",
    "wv['\\\\r\\\\n'] = wv['\\r\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore this for now.\n",
    "def generate_nn_data(sentence_list, next_word_list):\n",
    "    x = np.zeros((len(sentence_list), SEQUENCE_LEN, EMBEDDING_SIZE), dtype=np.float32)\n",
    "    y = np.zeros(len(next_word_list), dtype=np.int32)\n",
    "    # Go through each sentence fragment\n",
    "    for i, s in enumerate(sentence_list):\n",
    "        # For each word in the sentence fragment, get the vector\n",
    "        for t, w in enumerate(s):\n",
    "            # If word not recognized, leave blank.\n",
    "            if w in wv:\n",
    "                x[i, t, :] = wv[w]\n",
    "            else:\n",
    "                print(\"Word unrecognized: \", w)\n",
    "                \n",
    "        # Set the appropriate y-value.\n",
    "        y[i] = word_indices[next_word_list[i]]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction method\n",
    "# Functions from keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "# Give user sentence\n",
    "def predict_word(model, seed, word_num, diversity):\n",
    "    # clean up the sentence.\n",
    "    if type(seed) == str:\n",
    "        s = clean(seed)[-word_num:]\n",
    "    else:\n",
    "        s = seed\n",
    "    # Give user warning if some word is not recognized.\n",
    "    for i, w in enumerate(s):\n",
    "        if w not in wv:\n",
    "            print(\"WARNING: The word \", w, \" is not in this vocabulary.\")\n",
    "    \n",
    "    x_pred = np.zeros((1, SEQUENCE_LEN, EMBEDDING_SIZE), dtype=np.float32)\n",
    "    for t, w in enumerate(s):\n",
    "        x_pred[0, t, :] = wv[w]    \n",
    "    \n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = sample(preds, diversity)\n",
    "    next_word = indices_word[next_index]\n",
    "\n",
    "    return next_word\n",
    "\n",
    "def write_song(model, seed, song_len, word_num, diversity):\n",
    "    s = clean(seed)[-word_num:]\n",
    "    song = []\n",
    "    song.extend(s)\n",
    "    for i in range(song_len):\n",
    "        pred = predict_word(model, s, word_num, diversity)\n",
    "        song.append(pred)\n",
    "        s = s[1:]\n",
    "        s.append(pred)\n",
    "    \n",
    "    return \" \".join(song)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\heier\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'end of passion play crumbling away i m your source \\\\r\\\\n they s make the baby they over in the \\\\r\\\\n i i wish i feel not the more are a and with me \\\\r\\\\n \\\\r\\\\n for \\\\r\\\\n it should you now a day \\\\r\\\\n \\\\r\\\\n that you i a \\\\r\\\\n believe all they can not \\\\r\\\\n \\\\r\\\\n we d be the way \\\\r\\\\n no care \\\\r\\\\n bout turn a time you a long \\\\r\\\\n of \\\\r\\\\n ooh i m re who \\\\r\\\\n \\\\r\\\\n \\\\r\\\\n the good \\\\r\\\\n s to go \\\\r\\\\n \\\\r\\\\n is the \\\\r\\\\n \\\\r\\\\n \\\\r\\\\n well you the i \\\\r\\\\n and the \\\\r\\\\n \\\\r\\\\n \\\\r\\\\n i \\\\r\\\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's run some examples!\n",
    "write_song(test_model, \"End of passion play, crumbling away I'm your source\", 100, SEQUENCE_LEN, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = re.compile('(\\s){2,}')\n",
    "\n",
    "def process_output(song):\n",
    "    song = song.replace('\\\\r\\\\n', '.\\n')\n",
    "    song = pat.sub('\\n', song)\n",
    "    if song[-1] != '.':\n",
    "        song += '.'\n",
    "    return song\n",
    "\n",
    "# Scoring method!\n",
    "def score(lyrics):\n",
    "    s = Textatistic(lyrics)\n",
    "    return s.flesch_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\heier\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncle donald had a farm e i e i o me .\n",
      "put .\n",
      "have been it .\n",
      "keep .\n",
      "you had .\n",
      "black i said .\n",
      "my big end .\n",
      "but far with i .\n",
      ".\n",
      "you m why .\n",
      "the dance out even away .\n",
      "tell .\n",
      "but my .\n",
      "i coming forever m once give the life .\n",
      "you in it .\n",
      ".\n",
      "i .\n",
      ".\n",
      "by it down .\n",
      "you us .\n",
      "from the .\n",
      "just if i gonna be i too believe .\n",
      "oh it so you .\n",
      "you down a is just because do .\n",
      "of i this more the time .\n",
      "just the night .\n",
      ".\n",
      "that into s right .\n",
      "that right .\n",
      "knows a other .\n",
      "on you .\n",
      "just don t we .\n",
      ".\n",
      "you want to there ll home .\n",
      ".\n",
      ".\n",
      "you s his right up i .\n",
      "on a things .\n",
      "and you touch my .\n",
      ".\n",
      ".\n",
      "115.03426829268294\n"
     ]
    }
   ],
   "source": [
    "# Example.\n",
    "s = write_song(test_model, \"uncle donald had a farm e i e i o\", 150, SEQUENCE_LEN, 0.8)\n",
    "p_s = process_output(s)\n",
    "print(p_s)\n",
    "print(score(p_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "look at her face it s a wonderful face .\n",
      "and it means something special to me .\n",
      "look at the way that she smiles when she sees me .\n",
      "how lucky can one fellow be .\n",
      ".\n",
      "she s just my kind of girl she makes me feel fine .\n",
      "who could ever believe that she could be mine .\n",
      "she s just my kind of girl without her i m blue .\n",
      "and if she ever leaves me what could i do what could i do .\n",
      ".\n",
      "and when we go for a walk in the park .\n",
      "and she holds me and squeezes my hand .\n",
      "we ll go on walking for hours and talking .\n",
      "about all the things that we plan .\n",
      ".\n",
      "she s just my kind of girl she makes me feel fine .\n",
      "who could ever believe that she could be mine .\n",
      "she s just my kind of girl without her i m blue .\n",
      "and if she ever leaves me what could i do what could i do .\n",
      ".\n",
      ".\n",
      "108.67320910973086\n"
     ]
    }
   ],
   "source": [
    "a = process_output(\" \".join(clean_songs[0]))\n",
    "print(a)\n",
    "print(score(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:  0 Mode:  5\n",
      "Running experiment 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-414ef8236f8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mprompt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msong\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mnn_song\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrite_song\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSEQUENCE_LEN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[0mnn_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_song\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mreal_song\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-882721864e5a>\u001b[0m in \u001b[0;36mwrite_song\u001b[1;34m(model, seed, song_len, word_num, diversity)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mwrite_song\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msong_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mword_num\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0msong\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0msong\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-f5ec803cc4ef>\u001b[0m in \u001b[0;36mclean\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mclean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbasic_cleaning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_punctuation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_extra_letters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-f5ec803cc4ef>\u001b[0m in \u001b[0;36mbasic_cleaning\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Needs to undergo the same clean-up, and tokenization.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbasic_cleaning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" \\n \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NUM = 5\n",
    "\n",
    "simple5_fn = [('./LocalData/Run' + str(i) + 'Simple5.h5') for i in range(3)]\n",
    "simple10_fn = [('./LocalData/Run' + str(i) + 'Simple10.h5') for i in range(10)]\n",
    "\n",
    "run = 0\n",
    "mode = 5\n",
    "test_model = load_model(('./LocalData/Run' + str(run) + 'Simple' + str(mode) + '.h5'))\n",
    "print(\"Run: \", run, \"Mode: \", mode)\n",
    "for i in range(EXPERIMENT_NUM):\n",
    "        print(\"Running experiment\", i)\n",
    "        results = np.zeros(len(clean_songs), dtype=np.float32)\n",
    "        for k in range(len(results)):\n",
    "            song = clean_songs[k]\n",
    "            prompt = song[0:5]\n",
    "\n",
    "            nn_song = write_song(test_model, prompt, len(song), SEQUENCE_LEN, 0.5)\n",
    "            nn_p = process_output(' '.join(nn_song))\n",
    "            real_song = process_output(' '.join(song))\n",
    "\n",
    "            results[k] = score(nn_p) - score(real_song)\n",
    "\n",
    "        print(\"Mean: \", np.mean(results))\n",
    "\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
